---
title: "The forecaster's toolbox"
author: "Notes from Forecasting: principles and practices"
output:
  html_document:
    toc: true
    df_print: paged
    code_folding: hide
---

## Simple forecasting methods

```{r}
require(pacman)
p_load(fpp2, tidyverse, lubridate, kableExtra)
data <- read_rds("../Datasets/CleanWeatherData.rds")
data <- data %>% filter(year(date)!=2011)
```

Creation of a time series per day:

```{r}
ts <- ts(data$ActiveEnergy, start = 2007, frequency = 365.25)
h <- 90
```

### Average method

```{r}
plot(meanf(ts, h)) # h is the forecast horizon, 90 days
```

### Naïve method 

```{r}
# set all forecasts to be the value of the last observation. This method works remarkably well for many economic and financial time series.
plot(naive(ts, h))
plot(rwf(ts, h))
# Because a naïve forecast is optimal when data follow a random walk (see Section 8.1), these are also called random walk forecasts.
```

### Seasonal naïve method

```{r}
# A similar method is useful for highly seasonal data. In this case, we set each forecast to be equal to the last observed value from the same season of the year
plot(snaive(ts, h))
# very good predictions 
```

### Drift method

```{r}
# A variation on the naïve method is to allow the forecasts to increase or decrease over time, where the amount of change over time (called the drift) is set to be the average change seen in the historical data. 
plot(rwf(ts, h, drift=TRUE))
```

### Ploting different forecasts

```{r}
# Plot some seasonal forecasts 
autoplot(ts) +
  autolayer(meanf(ts, h),
            series="Mean", PI=FALSE) +
  autolayer(naive(ts, h),
            series="Naïve", PI=FALSE) +
  autolayer(snaive(ts, h),
            series="Seasonal naïve", PI=FALSE) +
  ggtitle("Forecasts for quarterly beer production") +
  xlab("Year") + ylab("Megalitres") +
  guides(colour=guide_legend(title="Forecast"))
```

```{r}
# Plot some non-seasonal forecasts 
autoplot(goog200) +
  autolayer(meanf(goog200, h=40),
            series="Mean", PI=FALSE) +
  autolayer(rwf(goog200, h=40),
            series="Naïve", PI=FALSE) +
  autolayer(rwf(goog200, drift=TRUE, h=40),
            series="Drift", PI=FALSE) +
  ggtitle("Google stock (daily ending 6 Dec 2013)") +
  xlab("Day") + ylab("Closing Price (US$)") +
  guides(colour=guide_legend(title="Forecast"))
```

## Transformation and adjustments

1. calendar adjustments
2. population adjustments
3. Inflation adjustments
4. Mathematical transformations

### Calendar adjustments

```{r}
# Some of the variation seen in seasonal data may be due to simple calendar effects. In such cases, it is usually much easier to remove the variation before fitting a forecasting model. The monthdays() function will compute the number of days in each month or quarter.

data %>% 
  mutate(year = year(date), month = month(date, label = T, locale = "us")) %>% group_by(year, month) %>% 
  summarise(mean = mean(ActiveEnergy)) -> data2

ts_month <- ts(data2[,3], start = 2007, frequency = 12)

ts_month_norm <- cbind(ts_month, 
                       DailyAverage = ts_month/monthdays(ts_month))
autoplot(ts_month_norm, facet=TRUE) +
  xlab("Years") + ylab("Watts/h") +
  ggtitle("Active energy")
```

### Mathematical tranformations

```{r}
# Mathematical transformations
lambda <- BoxCox.lambda(ts_month)
autoplot(BoxCox(ts_month, lambda))
```

## Residual diagnosis

Proporties of the residuals:

1. The residuals are uncorrelated. If there are correlations between residuals, then there is information left in the residuals which should be used in computing forecasts.
2. The residuals have zero mean. If the residuals have a mean other than zero, then the forecasts are biased.

Any forecasting method that does not satisfy these properties can be improved. Also has the following properties:

3. The residuals have constant variance.
4. The residuals are normally distributed.

### Example non seasonal data

For stock market prices and indexes, the best forecasting method is often the naïve method. 

```{r}
t <- ts_month
autoplot(t) +
  xlab("Time") + ylab("Watts/h") +
  ggtitle("Avg. energy consumped by day")
```

Let's check at the errors using the naïve method, :

```{r}
autoplot(snaive(t, h = 18))

res <- residuals(snaive(t, h = 18))
autoplot(res) +
  xlab("Time") + ylab("Watts/h") +
  ggtitle("Avg. energy consumped by day")
```

Understanding the errors distribution through histograms:

```{r}
gghistogram(res, bins = 30) + ggtitle("Histogram of residuals")
```

Looking for errors in the distribution of the data, we can see there are a lot of errors and they have correlatiob between them:

```{r}
ggAcf(res) + ggtitle("ACF of residuals")
```

### Using Portmanteau tests for autocorrelation

```{r}
checkresiduals(naive(t))
```

## Training and tests sets

The size of the test set is typically about 20% of the total sample, although this value depends on how long the sample is and how far ahead you want to forecast. The test set should ideally be at least as large as the maximum forecast horizon required. The following points should be noted.

- A model which fits the training data well will not necessarily forecast well.
- A perfect fit can always be obtained by using a model with enough parameters.
- Over-fitting a model to data is just as bad as failing to identify a systematic pattern in the data.

Functions to subset time series:

- window:

```{r}
window(t, start = 2010)
```

- subset:

```{r}
subset(t, start = length(t)-4*5) # select 1/5 of the data
```

Or using subset function to extract values for a specific month, for the whole years:

```{r}
subset(t, month = 1)
```

- head and tail()

```{r}
tail(t, 4*5)
```

### Forecast errors

We can measure forecast accuracy by summarising the forecast errors in different ways:

#### Scale-dependent errors

The two most commonly used scale-dependent measures are based on the absolute errors or squared errors:

* Mean absolute error: MAE

* Root mean squared error: RMSE

When comparing forecast methods applied to a single time series, or to several time series with the same units, the MAE is popular as it is easy to both understand and compute. A forecast method that minimises the MAE will lead to forecasts of the median, while minimising the RMSE will lead to forecasts of the mean. Consequently, the RMSE is also widely used, despite being more difficult to interpret.

#### Percentage errors

Percentage errors have the advantage of being unit-free, and so are frequently used to compare forecast performances between data sets. The most commonly used measure is:

* Mean absolute percentage error: MAPE

Another problem with percentage errors that is often overlooked is that they assume the unit of measurement has a meaningful zero.2 For example, a percentage error makes no sense when measuring the accuracy of temperature forecasts on either the Fahrenheit or Celsius scales, because temperature has an arbitrary zero point.

They also have the disadvantage that they put a heavier penalty on negative errors than on positive errors. This observation led to the use of the so-called “symmetric” MAPE (sMAPE) proposed by Armstrong (1978, p. 348), which was used in the M3 forecasting competition. It is defined by:

* sMAPE

Hyndman & Koehler (2006) recommend that the sMAPE not be used. It is included here only because it is widely used, although we will not use it in this book.

#### Scaled errors

The mean absolute scaled error is simply MASE.

```{r}
t2 <- window(t,start=2007,end=c(2009,10))
fit1 <- meanf(t2,h=12)
fit2 <- rwf(t2,h=12)
fit3 <- snaive(t2,h=12)
autoplot(window(t2, start=2007)) +
  autolayer(fit1, series="Mean", PI=FALSE) +
  autolayer(fit2, series="Naïve", PI=FALSE) +
  autolayer(fit3, series="Seasonal naïve", PI=FALSE) +
  xlab("Time") + ylab("Watts/h") +
  ggtitle("Avg. energy consumped by day") +
  guides(colour=guide_legend(title="Forecast"))
```

Let's see the metric results:

```{r}
t3 <- window(t, start=c(2009,10))
accuracy(fit1, t3) # meanf
accuracy(fit2, t3) # rwf, drift method
accuracy(fit3, t3) # snaïve
```

#### Non seasonal example

```{r}
googfc1 <- meanf(goog200, h=40)
googfc2 <- rwf(goog200, h=40)
googfc3 <- rwf(goog200, drift=TRUE, h=40)
autoplot(subset(goog, end = 240)) +
  autolayer(googfc1, PI=FALSE, series="Mean") +
  autolayer(googfc2, PI=FALSE, series="Naïve") +
  autolayer(googfc3, PI=FALSE, series="Drift") +
  xlab("Day") + ylab("Closing Price (US$)") +
  ggtitle("Google stock price (daily ending 6 Dec 13)") +
  guides(colour=guide_legend(title="Forecast"))
```

```{r}
googtest <- window(goog, start=201, end=240)
accuracy(googfc1, googtest)
accuracy(googfc2, googtest) 
accuracy(googfc3, googtest)
```

## Time series cross-validation

```{r}
e <- tsCV(goog200, rwf, drift=TRUE, h=1)
sqrt(mean(e^2, na.rm=TRUE))
#> [1] 6.233 # RMSE with cross validation
sqrt(mean(residuals(rwf(goog200, drift=TRUE))^2, na.rm=TRUE))
#> [1] 6.169 # RMSE without cross validation
```

As expected, the RMSE from the residuals is smaller, as the corresponding “forecasts” are based on a model fitted to the entire data set, rather than being true forecasts.

A good way to choose the best forecasting model is to find the model with the smallest RMSE computed using time series cross-validation.

### Pipe operator

We can do the same thing than above but with pipes operators:

```{r}
goog200 %>% tsCV(forecastfunction=rwf, drift=TRUE, h=1) -> e
e^2 %>% mean(na.rm=TRUE) %>% sqrt()
#> [1] 6.233
goog200 %>% rwf(drift=TRUE) %>% residuals() -> res
res^2 %>% mean(na.rm=TRUE) %>% sqrt()
#> [1] 6.169
```

### Example: using tsCV()

The code below evaluates the forecasting performance of 1- to 8-step-ahead naïve forecasts with tsCV(), using MSE as the forecast error measure. The plot shows that the forecast error increases as the forecast horizon increases, as we would expect.

```{r}
e <- tsCV(goog200, forecastfunction=naive, h=8)
# Compute the MSE values and remove missing values
mse <- colMeans(e^2, na.rm = T)
# Plot the MSE values against the forecast horizon
data.frame(h = 1:8, MSE = mse) %>%
  ggplot(aes(x = h, y = MSE)) + geom_point()
```

## The forecast package in R

Main functions included into the library fpp2 (caontains the forecast package):

- meanf()
- naive(), snaive()
- rwf()
- croston()
- stlf()
- ses()
- holt(), hw()
- splinef()
- thetaf()
- forecast() -> function to produce forecasts from that model.

# Exercices 

### 1. For the following series, find an appropriate Box-Cox transformation in order to stabilise the variance 

##### usnetelec {.tabset}

###### autoplot

```{r .tabset}
autoplot(usnetelec)
```

###### autoplot with lambda to stabilise the variance

```{r}
# we are applying a tranformation but it doesn't need it
lambda <- BoxCox.lambda(usnetelec)
autoplot(BoxCox(usnetelec, lambda))
```

##### usgdp {.tabset}

###### autoplot usgdp

```{r}
autoplot(usgdp)
```

###### variance normalization with lambda

```{r}
# we are applying a tranformation but it doesn't need it 
lambda <- BoxCox.lambda(usgdp)
autoplot(BoxCox(usgdp, lambda))
```

##### mcopper {.tabset}

Monthly copper prices. Copper, grade A, electrolytic wire bars/cathodes,LME,cash (pounds/ton) 

###### autoplot

```{r}
autoplot(mcopper)
```

###### variance stabilization with lambda

```{r}
lambda <- BoxCox.lambda(mcopper)
autoplot(BoxCox(mcopper, lambda = 0))
```

##### enplanements {.tabset}

"Domestic Revenue Enplanements (millions): 1996-2000. SOURCE: Department of Transportation, Bureau of Transportation Statistics, Air Carrier Traffic Statistic Monthly.

###### autoplot 

```{r}
autoplot(enplanements)
```

###### variance stabilization with lambda

```{r}
# We will do a log transformation
enplanements %>% BoxCox(lambda = 0) %>% autoplot()
```

### 2. Why is a Box-Cox transformation unhelpful for the cangas data?

##### cangas dataset {.tabset}

cangas = Monthly Canadian gas production, billions of cubic metres, January 1960 - February 2005

###### autoplot cangas

```{r}
autoplot(cangas)
```

###### applying lambda to deal with variance

```{r}
autoplot(BoxCox(cangas, lambda = BoxCox.lambda(cangas)))
```

Here the variance of the series changes, but not with the level of the series. Box Cox transformations are designed to handle series where the variance increases (or decreases) with the level of the series.

### 3. What Box-Cox transformation would you select for your retail data (from Exercise 3 in Section 2.10)?

##### working with retail data {.tabset}

###### autplot

```{r}
retaildata <- readxl::read_xlsx("../Datasets/practice_datasets/retail.xlsx", skip = 1)
myts <- ts(retaildata[,"A3349873A"], start = c(1982,4), frequency = 12)
autoplot(myts)
```

###### checking boxcox to deal with variance

From visual inspection, a log transformation would be appropriate here. It also makes sense, as retail expenditure is likely to increase proportionally to population, and therefore the seasonal fluctuations are likely to be proportional to the level of the series. It has the added advantage of being easier to explain than some other transformations. Finally, it is relatively close to the automatically selected value of `BoxCox.lambda(myts)` = `r BoxCox.lambda(myts)`.

If you have selected a different series from the retail data set, you might choose a different transformation.

```{r}
lambda = 0
autoplot(BoxCox(myts, lambda = lambda))
```

### 4. For each of the following series, make a graph of the data. If transforming seems appropriate, do so and describe the effect

##### dole {.tabset}

Monthly total of people on unemployment benefits in Australia (Jan 1965 – Jul 1992).

###### no transformation

```{r}
autoplot(dole)
```

###### transformation

```{r}
lambda <- BoxCox.lambda(dole)
dole %>% BoxCox(lambda) %>% autoplot() + ylab(paste("BoxCox(# people,", round(lambda, 2), ")"))
```

##### usdeaths {.tabset}

Monthly accidental deaths in USA.

###### no transformation

The data was transformed using Box-Cox transformation with parameter λ=0.33. The transformation has stabilized the variance.

```{r}
autoplot(usdeaths)
```

###### transformation

```{r}
lambda <- BoxCox.lambda(usdeaths)
usdeaths %>% BoxCox(lambda) %>% autoplot() + ylab(paste("death, lambda =", round(lambda, 2)))
```

##### bricksq {.tabset}

Australian quarterly clay brick production: 1956–1994.

###### transformation 

```{r}
autoplot(bricksq)
```

###### no transformation

The time series was transformed using a Box-Cox transformation with λ=0.25. The transformation has stabilized the variance.

```{r}
lambda <- BoxCox.lambda(bricksq)
bricksq %>% BoxCox(lambda) %>% autoplot() + ylab(paste("BoxCox(# mln bricks, lambda =", round(lambda, 2), ")"))
```

### 5. Calculate the residuals from a seasonal naïve forecast applied to the quarterly Australian beer production data from 1992. The following code will help.

##### plot and visualization {.tabset}

Monthly Australian beer production: Jan 1991 – Aug 1995.

```{r}
beer <- window(ausbeer, start=1992)
```

###### Seasonal naive method

```{r}
fc <- snaive(beer)
autoplot(fc)
```

###### ploting errors

```{r}
res <- residuals(fc)
autoplot(res)
```

##### Checking residuals

Test if the residuals are white noise and normally distributed.

```{r}
checkresiduals(fc)
```

What do you conclude?

The residuals are correlated: the Null of no joint autocorrelation is clearly rejected. We can also see a significant spike on the seasonal (4th lag) in the ACF. There is considerable information remaining in the residuals which has not been captured with the seasonal naïve method. The residuals do not appear to be too far from Normally distributed.

### 6. Repeat the exercise for the WWWusage and bricksq data. Use whichever of naive() or snaive() is more appropriate in each case.

##### WWWusage {.tabset}

A time series of the numbers of users connected to the Internet through a server every minute.

###### autoplot

```{r}
autoplot(WWWusage)
```

###### pred: naive method

```{r}
fr_n <- naive(WWWusage, h = 15)
autoplot(fr_n)
```

###### error analysis naïve

Residuals are correlated as shown by both the LB test and the ACF. They seem to be normally distributed. There is considerable information remaining in the residuals which has not been captured with the naïve method.

```{r}
checkresiduals(fr_n)
```

##### bricksq {.tabset}

Australian quarterly clay brick production: 1956–1994.

###### autoplot

```{r}
autoplot(bricksq)
```

###### pred: seasonal naive method

```{r}
fr_sn <- snaive(bricksq)
autoplot(fr_sn)
```

###### err. analysis seas.naïve

Residuals are correlated as shown by both the LB test and the ACF and do not appear to be normal (they have a long left tail). There is considerable information remaining in the residuals which has not been captured with the seasonal naïve method.

```{r}
checkresiduals(fr_sn)
```

### 7. Are the following statements true or false? Explain your answer.

a. Good forecast methods should have normally distributed residuals. FALSE

Not true. It is helpful to have normally distributed residuals because it makes the calculation of prediction intervals easier, and it means that least squares estimates of parameters are also equivalent (or close to) maximum likelihood estimates. But it doesn’t make the forecasts better. If the residuals are not normally-distributed, one way to produce prediction intervals is to use a bootstrapped approach.

b. A model with small residuals will give good forecasts. FALSE

Not true. An over-fitted model will have small residuals (relative to other models), but will probably not give good forecasts.

c. The best measure of forecast accuracy is MAPE. FALSE

Not true. MAPE is useful in some circumstances — for example, in comparing forecasts on different scales, or with different units, and is relatively easy to understand. But it is not appropriate if the data includes zeros, or if the data has no natural zero.

d. If your model doesn’t forecast well, you should make it more complicated. FALSE

Not true. Some things are hard to forecast, and making a model more complicated can make the forecasts worse.

e. Always choose the model with the best forecast accuracy as measured on the test set. FALSE

Not true. Imagine the test set has only a single observation, or a very small number of observations. We don’t want to select a model based on a small test set. A better approach is to use time-series cross-validation, which is based on a much larger set of test sets. Later, we will learn about the AIC statistic which is an alternative way to select a model and is often more helpful than a simple test set.

### 8. For your retail time series (from Exercise 3 in Section 2.10):

a. Split the data into two parts using

```{r}
myts.train <- window(myts, end=c(2010,12))
myts.test <- window(myts, start=2011)
```

b. Check that your data have been split appropriately by producing the following plot.

```{r}
autoplot(myts) +
  autolayer(myts.train, series="Training") +
  autolayer(myts.test, series="Test")
```

c. Calculate forecasts using snaive applied to myts.train.

```{r}
fc <- snaive(myts.train)
autoplot(fc)
```

d. Compare the accuracy of your forecasts against the actual values stored in myts.test.

```{r}
kable(accuracy(fc,myts.test), format = "markdown")
```

e. Check the residuals.

```{r}
checkresiduals(fc)
```

The residuals do not look like white noise there are lots of dynamics left over that need to be explored. They do look close to normal although the tails may be too long.

Do the residuals appear to be uncorrelated and normally distributed?

Yes, they do

f. How sensitive are the accuracy measures to the training/test split?

The accuracy measure are always sensitive to this split. There are better ways to check the robustness of the methods in terms of accuracy such as using a rolling window (possible in this case as we have lots of data) or ts.cv.


### 9. "visnights" contains quarterly visitor nights (in millions) from 1998 to 2016 for twenty regions of Australia.

```{r}
autoplot(visnights)
stats::start(visnights)
stats::end(visnights)
stats::frequency(visnights)
```

##### a. Use window() to create three training sets for visnights[,"QLDMetro"], omitting the last 1, 2 and 3 years; call these train1, train2, and train3, respectively. For example: {.tabset}

###### end(2015, 4)

```{r}
train1 <- window(visnights[, "QLDMetro"], end = c(2015, 4))
test1 <- window(visnights[, "QLDMetro"], start = c(2016, 1))
autoplot(train1)
```

###### end(2014, 4)

```{r}
train2 <- window(visnights[, "QLDMetro"], end = c(2014, 4))
test2 <- window(visnights[, "QLDMetro"], start = c(2015, 1))
autoplot(train2)
```

###### end(2013, 4)

```{r}
train3 <- window(visnights[, "QLDMetro"], end = c(2013, 4))
test3 <- window(visnights[, "QLDMetro"], start = c(2014, 1))
autoplot(train3)
```

##### b. Compute one year of forecasts for each training set using the snaive() method. Call these fc1, fc2 and fc3, respectively. {.tabset}

###### 1y prediction

```{r}
fc1 <- snaive(train1, h = 4)
autoplot(fc1)
```

###### 2y prediction

```{r}
fc2 <- snaive(train2, h = 8)
autoplot(fc2)
```

###### 3y prediction

```{r}
fc3 <- snaive(train3, h = 12)
autoplot(fc3)
```

##### c. Use accuracy() to compare the MAPE over the three test sets. Comment on these. {.tabset}

###### predictions 1y

```{r}
kable(accuracy(fc1, test1), format = "markdown")
```

###### predictions 2y

The lower MAPE value for "fc3" indicates a better result when er use the previous 3 values for the snaive() prediction.

```{r}
kable(accuracy(fc2, test2), format = "markdown")
```

###### predictions 3y

```{r}
kable(accuracy(fc3, test3), format = "markdown")
```

### 10. Use the Dow Jones index (data set "dowjones") to do the following:

##### a. Produce a time plot of the series

```{r}
autoplot(dowjones)
start(dowjones)
end(dowjones)
frequency(dowjones)
```

##### b. Produce forecasts using the drift method and plot them.

```{r}
fc.dj1 <- rwf(dowjones, h = 5, drift = T)
fc.dj2 <- rwf(dowjones, h = 10, drift = T)
fc.dj3 <- rwf(dowjones, h = 15, drift = T)
```

##### c. Show that the forecasts are identical to extending the line drawn between the first and last observations.

```{r}
autoplot(dowjones) +
  autolayer(fc.dj3, PI = F, series = "Drift 15") +
  autolayer(fc.dj2, PI = F, series = "Drift 10") +
  autolayer(fc.dj1, PI = F, series = "Drift 5") +
  xlab("Time") + ylab("Closing Price (US$)") +
  ggtitle("Dow Jones index") +
  guides(colour=guide_legend(title="Forecast"))
```

##### d. Try using some of the other benchmark functions to forecast the same data set. Which do you think is best? Why?

The time series isn’t seasonal, so the seasonal naïve method is not viable. However, we can use the mean and naïve methods. 

The three values will be very different here. The Mean will use the data set, so is unlikely to follow the current trendline.

```{r}
fc.dw1 <- meanf(dowjones, h = 5)
fc.dw2 <- rwf(dowjones, h = 5)
fc.dw3 <- rwf(dowjones, drift = T, h = 5)
autoplot(dowjones) +
  autolayer(fc.dw1, PI = F, series = "Mean") +
  autolayer(fc.dw2, PI = F, series = "Naïve") +
  autolayer(fc.dw3, PI = F, series = "Drift") +
  xlab("Time") + ylab("Closing Price (US$)") +
  ggtitle("Dow Jones index") +
  guides(colour=guide_legend(title="Forecast"))
```

### 11. Consider the daily closing IBM stock prices (data set "ibmclose").

##### a. Produce some plots of the data in order to become familiar with it.

```{r}
autoplot(ibmclose)
```

##### b. Split the data into a training set of 300 observations and a test set of 69 observations.

green = training
red = testing

```{r}
train <- window(ibmclose, end = 300)
test <- window(ibmclose, start = 301)
autoplot(ibmclose) +
  autolayer(train, series = "Training") +
  autolayer(test, series = "Testing") +
  xlab("Time") + ylab("Closing Price (US$)") +
  ggtitle("Ibm stocks colse") +
  guides(colour=guide_legend(title="Forecast")) 
```

##### c. Try using various benchmark methods to forecast the training set and compare the results on the test set. Which method did best?

```{r}
# define the h paramether
h <- length(test)
fc.mf <- meanf(train, h = h)
fc.dr1 <- rwf(train, h = h)
fc.dr2 <- rwf(train, drift = T, h = h)

autoplot(train) +
  autolayer(fc.mf, PI = F,series = "Meanf") +
  autolayer(fc.dr1, PI = F, series = "Naïve method") +
  autolayer(fc.dr2, PI = F, series = "Drift method") +
  autolayer(test, alpha = 0.5) +
  xlab("Day") + ylab("Closing Price (US$)") +
  ggtitle("Daily closing IBM stock prices") 
```

##### Forecast results {.tabset}

###### meanf

```{r}
kable(accuracy(fc.mf, test), format = "markdown")
```

###### Naïve method

```{r}
kable(accuracy(fc.dr1, test), format = "markdown")
```

###### Drift method

```{r}
kable(accuracy(fc.dr2, test), format = "markdown")
```

In terms of accuracy measures on the test set, the drift method does better.

##### d. Check the residuals of your preferred method. Do they resemble white noise?

Residuals look relatively uncorrelated, but the distribution is not normal (tails too long).

```{r}
checkresiduals(fc.dr2)
```

```{r}
Box.test(residuals(fc.dr2), 
         type="Lj", #test to be performed: partial matching is used.
         lag = 10, #the statistic will be based on lag autocorrelation coefficients.
         fitdf=1) #number of degrees of freedom to be subtracted if x is a series of residuals.
```

### 12. Consider the sales of new one-family houses in the USA, Jan 1973 – Nov 1995 (data set hsales).

##### a. Produce some plots of the data in order to become familiar with it.

```{r}
autoplot(hsales) + ylab("Sales")
```

##### b. Split the "hsales" data set into a training set and a test set, where the test set is the last two years of data.

```{r}
train <- window(hsales, end = c(1993, 11))
test <- window(hsales, start = c(1993, 12))
autoplot(train, series = "train") +
  autolayer(test, series = "test")
```

##### c. Try using various benchmark methods to forecast the training set and compare the results on the test set. Which method did best?

```{r}
h <- length(test)
m.f <- meanf(train, h=h)
rw.f <- rwf(train, h=h)
sn.f <- snaive(train, h=h)
rwd.f <- rwf(train, drift=TRUE, h=h)

autoplot(train) +
  xlab("Year") +
  ggtitle("Sales") +
  autolayer(m.f$mean, col=2, series="Mean method") +
  autolayer(rw.f$mean, col=3, series="Naïve method") +
  autolayer(sn.f$mean, col=4, series="Seasonal naïve method") +
  autolayer(rwd.f$mean, col=5, series="Drift method") +
  autolayer(test, alpha = 0.30)
```

##### Forecast results {.tabset}

###### meanf

```{r}
kable(accuracy(m.f, test), format = "markdown")
```

###### Naïve method

```{r}
kable(accuracy(rw.f, test), format = "markdown")
```

###### Seasonal naïve method

```{r}
kable(accuracy(sn.f, test), format = "markdown")
```

###### Drift method

```{r}
kable(accuracy(rwd.f, test), format = "markdown")
```

##### d. Check the residuals of your preferred method. Do they resemble white noise?

```{r}
checkresiduals(sn.f)
```

